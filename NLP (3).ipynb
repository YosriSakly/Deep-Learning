{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYCTXGXXx0n6",
        "colab_type": "code",
        "outputId": "9a87ad73-d0bd-4002-92e5-4d8f9101ed4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "!gzip -d GoogleNews-vectors-negative300.bin.gz "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 13:25:19--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.104.133\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.104.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  82.7MB/s    in 18s     \n",
            "\n",
            "2019-06-07 13:25:37 (86.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6iCgDR90buz",
        "colab_type": "code",
        "outputId": "23a9fe40-fd93-454a-b38f-01337dc08f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "model =  gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "words= (list(model.vocab.keys()))\n",
        "embeddings = model.vectors\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-JBXJEj4MzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POSITIVE_FINANCE = [\"bullish\",\"climb\",\"surge\",\"hortation\",\"successful\", \"excellent\", \"profit\", \"beneficial\", \"improving\", \"improved\", \"success\", \"gains\", \"positive\"]\n",
        "# NEGATIVE_FINANCE = [\"bearish\",\"fall\",\"slump\",\"negligent\", \"sanction\",\"loss\", \"volatile\", \"wrong\", \"losses\", \"damages\", \"bad\", \"litigation\", \"failure\", \"down\", \"negative\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbvvcmkqvzJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSITIVE_FINANCE = [\"successful\", \"excellent\", \"profit\", \"beneficial\", \"improving\", \"improved\", \"success\", \"gains\", \"positive\"]\n",
        "NEGATIVE_FINANCE = [\"negligent\", \"loss\", \"volatile\", \"wrong\", \"losses\", \"damages\", \"bad\", \"litigation\", \"failure\", \"down\", \"negative\"]\n",
        "\n",
        "def finance_seeds():\n",
        "    return POSITIVE_FINANCE, NEGATIVE_FINANCE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG84qrlavzKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from itertools import chain\n",
        "from nltk.corpus import wordnet as wn\n",
        "import multiprocessing\n",
        "\n",
        "MAX_PROCS=8\n",
        "\n",
        "\"\"\"\n",
        "Methods for constructing graphs from word embeddings.\n",
        "\"\"\"\n",
        "\n",
        "def similarity_matrix(embeddings, arccos=False, similarity_power=1, nn=20, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a similarity matrix from embeddings.\n",
        "    nn argument controls the degree.\n",
        "    \"\"\"\n",
        "    def make_knn(vec, nn=nn):\n",
        "        vec[vec < vec[np.argsort(vec)[-nn]]] = 0\n",
        "        return vec\n",
        "    L = embeddings.m.dot(embeddings.m.T)\n",
        "    if sparse.issparse(L):\n",
        "        L = L.todense()\n",
        "    if arccos:\n",
        "        L = np.arccos(np.clip(-L, -1, 1))/np.pi\n",
        "    else:\n",
        "        L += 1\n",
        "    np.fill_diagonal(L, 0)\n",
        "    L = np.apply_along_axis(make_knn, 1, L)\n",
        "    return L ** similarity_power\n",
        "\n",
        "def transition_matrix(embeddings, word_net=False, first_order=False, sym=False, trans=False, **kwargs):\n",
        "    \"\"\"\n",
        "    Build a probabilistic transition matrix from word embeddings.\n",
        "    \"\"\"\n",
        "    if word_net:\n",
        "        L =  wordnet_similarity_matrix(embeddings)\n",
        "    elif not first_order:\n",
        "        L = similarity_matrix(embeddings, **kwargs)\n",
        "    else:\n",
        "        L = embeddings.m.todense().A\n",
        "    if sym:\n",
        "        Dinv = np.diag([1. / np.sqrt(L[i].sum()) if L[i].sum() > 0 else 0 for i in range(L.shape[0])])\n",
        "        return Dinv.dot(L).dot(Dinv)\n",
        "    else:\n",
        "        Dinv = np.diag([1. / L[i].sum() for i in range(L.shape[0])])\n",
        "        L = L.dot(Dinv)\n",
        "    if trans:\n",
        "        return L.T\n",
        "    return L\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8uZU_z2vzKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import heapq\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "    \"\"\"\n",
        "    Base class for all embeddings. SGNS can be directly instantiated with it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vecs, vocab, normalize=True, **kwargs):\n",
        "        self.m = vecs\n",
        "        self.dim = self.m.shape[1]\n",
        "        self.iw = vocab\n",
        "        self.wi = {w:i for i,w in enumerate(self.iw)}\n",
        "        if normalize:\n",
        "            self.normalize()\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if self.oov(key):\n",
        "            raise KeyError\n",
        "        else:\n",
        "            return self.represent(key)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.iw.__iter__()\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return not self.oov(key)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path, normalize=True, add_context=True, **kwargs):\n",
        "        mat = np.load(path + \"-w.npy\")\n",
        "        if add_context:\n",
        "            mat += np.load(path + \"-c.npy\")\n",
        "        iw = load_pickle(path + \"-vocab.pkl\")\n",
        "        return cls(mat, iw, normalize) \n",
        "\n",
        "    def get_subembed(self, word_list, **kwargs):\n",
        "        word_list = [word for word in word_list if not self.oov(word)]\n",
        "        keep_indices = [self.wi[word] for word in word_list]\n",
        "        return Embedding(self.m[keep_indices, :], word_list, normalize=False)\n",
        "\n",
        "    def reindex(self, word_list, **kwargs):\n",
        "        new_mat = np.empty((len(word_list), self.m.shape[1]))\n",
        "        valid_words = set(self.iw)\n",
        "        for i, word in enumerate(word_list):\n",
        "            if word in valid_words:\n",
        "                new_mat[i, :] = self.represent(word)\n",
        "            else:\n",
        "                new_mat[i, :] = 0 \n",
        "        return Embedding(new_mat, word_list, normalize=False)\n",
        "\n",
        "    def get_neighbourhood_embed(self, w, n=1000):\n",
        "        neighbours = self.closest(w, n=n)\n",
        "        keep_indices = [self.wi[neighbour] for _, neighbour in neighbours] \n",
        "        new_mat = self.m[keep_indices, :]\n",
        "        return Embedding(new_mat, [neighbour for _, neighbour in neighbours]) \n",
        "\n",
        "    def normalize(self):\n",
        "        preprocessing.normalize(self.m, copy=False)\n",
        "\n",
        "    def oov(self, w):\n",
        "        return not (w in self.wi)\n",
        "\n",
        "    def represent(self, w):\n",
        "        if w in self.wi:\n",
        "            return self.m[self.wi[w], :]\n",
        "        else:\n",
        "            print (\"OOV: \", w)\n",
        "            return np.zeros(self.dim)\n",
        "\n",
        "    def similarity(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Assumes the vectors have been normalized.\n",
        "        \"\"\"\n",
        "        sim = self.represent(w1).dot(self.represent(w2))\n",
        "        return sim\n",
        "\n",
        "    def closest(self, w, n=10):\n",
        "        \"\"\"\n",
        "        Assumes the vectors have been normalized.\n",
        "        \"\"\"\n",
        "        scores = self.m.dot(self.represent(w))\n",
        "        return heapq.nlargest(n, zip(scores, self.iw))\n",
        "    \n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "class SVDEmbedding(Embedding):\n",
        "    \"\"\"\n",
        "    SVD embeddings.\n",
        "    Enables controlling the weighted exponent of the eigenvalue matrix (eig).\n",
        "    Context embeddings can be created with \"transpose\".\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed,words,dim=300, normalize=True, eig=0.0, **kwargs):\n",
        "        ut, s, v = randomized_svd(embed, n_components=dim, n_iter=5)\n",
        "        self.iw = words\n",
        "        self.wi = {w:i for i, w in enumerate(self.iw)}\n",
        " \n",
        "        if eig == 0.0:\n",
        "            self.m = ut\n",
        "        elif eig == 1.0:\n",
        "            self.m = s * ut\n",
        "        else:\n",
        "            self.m = np.power(s, eig) * ut\n",
        "\n",
        "        self.dim = self.m.shape[1]\n",
        "\n",
        "        if normalize:\n",
        "            self.normalize()\n",
        "\n",
        "class GigaEmbedding(Embedding):\n",
        "    def __init__(self, embed, words, dim=300, normalize=True, **kwargs):\n",
        "        self.iw = words\n",
        "        self.wi = {w:i for i,w in enumerate(self.iw)}\n",
        "        self.m = np.vstack(w for w in embed)\n",
        "        if normalize:\n",
        "            self.normalize()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6xVIkUvzKE",
        "colab_type": "code",
        "outputId": "89eeb56f-c1bf-4e7e-aa48-56092fdc3dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations, product\n",
        "from keras import backend as K\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers.core import Dense, Lambda\n",
        "from keras.optimizers import Adam, Optimizer\n",
        "from keras.regularizers import Regularizer\n",
        "from keras.constraints import Constraint\n",
        "import theano.tensor as T\n",
        "from keras.layers import Input, Dense\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\"\"\"\n",
        "Helper methods for learning transformations of word embeddings.\n",
        "\"\"\"\n",
        "\n",
        "class SimpleSGD(Optimizer):\n",
        "    def __init__(self, lr=5, momentum=0., decay=0.,\n",
        "                 nesterov=False, **kwargs):\n",
        "        super(SimpleSGD, self).__init__(**kwargs)\n",
        "        self.__dict__.update(locals())\n",
        "        self.iterations = K.variable(0.)\n",
        "        self.lr = K.variable(lr)\n",
        "        self.momentum = K.variable(momentum)\n",
        "        self.decay = K.variable(decay)\n",
        "\n",
        "    def get_updates(self, params, loss):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        lr = self.lr * 0.99\n",
        "        self.updates = [(self.iterations, self.iterations + 1.)]\n",
        "\n",
        "        # momentum\n",
        "        self.weights = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n",
        "        for p, g, m in zip(params, grads, self.weights):\n",
        "            v = self.momentum * m - lr * g  # velocity\n",
        "            self.updates.append((m, v))\n",
        "\n",
        "            if self.nesterov:\n",
        "                new_p = p + self.momentum * v - lr * g\n",
        "            else:\n",
        "                new_p = p + v\n",
        "\n",
        "            self.updates.append((p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'momentum': float(K.get_value(self.momentum)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'nesterov': self.nesterov}\n",
        "        base_config = super(SimpleSGD, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "class Orthogonal(Constraint):\n",
        "    def __call__(self, p):\n",
        "        print (\"here\")\n",
        "        u,s,v = T.nlinalg.svd(p)\n",
        "        return K.dot(u,K.transpose(v))\n",
        "\n",
        "class OthogonalRegularizer(Regularizer):\n",
        "    def __init__(self, strength=0.):\n",
        "        self.strength = strength\n",
        "\n",
        "    def set_param(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, loss):\n",
        "        loss += K.sum(K.square(self.p.dot(self.p.T) - T.identity_like(self.p))) * self.strength\n",
        "        return loss\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"name\": self.__class__.__name__,\n",
        "                \"strength\": self.strength}\n",
        "\n",
        "\n",
        "def orthogonalize(Q):\n",
        "    U, S, V = np.linalg.svd(Q)\n",
        "    return U.dot(V.T)\n",
        "\n",
        "\n",
        "class DatasetMinibatchIterator:\n",
        "    def __init__(self, embeddings, positive_seeds, negative_seeds, batch_size=512, **kwargs):\n",
        "        self.words, embeddings1, embeddings2, labels = [], [], [], []\n",
        "\n",
        "        def add_examples(word_pairs, label):\n",
        "            for w1, w2 in word_pairs:\n",
        "                embeddings1.append(embeddings[w1])\n",
        "                embeddings2.append(embeddings[w2])\n",
        "                labels.append(label)\n",
        "                self.words.append((w1, w2))\n",
        "\n",
        "        add_examples(combinations(positive_seeds, 2), 1)\n",
        "        add_examples(combinations(negative_seeds, 2), 1)\n",
        "        add_examples(product(positive_seeds, negative_seeds), -1)\n",
        "        self.e1 = np.vstack(embeddings1)\n",
        "        self.e2 = np.vstack(embeddings2)\n",
        "        self.y = np.array(labels)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_batches = (self.y.size + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def shuffle(self):\n",
        "        perm = np.random.permutation(np.arange(self.y.size))\n",
        "        self.e1, self.e2, self.y, self.words = \\\n",
        "            self.e1[perm], self.e2[perm], self.y[perm], [self.words[i] for i in perm]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.n_batches):\n",
        "            batch = np.arange(i * self.batch_size, min(self.y.size, (i + 1) * self.batch_size))\n",
        "            yield {\n",
        "                'embeddings1': self.e1[batch],\n",
        "                'embeddings2': self.e2[batch],\n",
        "                'y': self.y[batch][:, np.newaxis]\n",
        "            }\n",
        "\n",
        "\n",
        "def get_model(inputdim, outputdim, regularization_strength=0.01, lr=0.000, cosine=False, **kwargs):\n",
        "  \n",
        "    \n",
        "    \n",
        "   \n",
        "    inputA = Input(name='embeddings1', shape=(inputdim,))\n",
        "    inputB = Input(name='embeddings2', shape=(inputdim,))\n",
        "    couche = Dense(inputdim, init='identity',\n",
        "                           W_constraint=Orthogonal())\n",
        "    transformed1 = couche(inputA)\n",
        "    transformed2 = couche( inputB)\n",
        "    projected1 = (Lambda(lambda x: x[:, :outputdim]))(transformed1)\n",
        "    negprojected2 = (Lambda(lambda x: -x[:, :outputdim]))(transformed2)\n",
        "    \n",
        "    if cosine:\n",
        "        w = Lambda(lambda x:  x / K.reshape(K.sqrt(K.sum(x * x, axis=1)),shape= (-1, 1)))(projected1)\n",
        "       \n",
        "        z = Lambda(lambda x:  x / K.reshape(K.sqrt(K.sum(x * x, axis=1)), shape = (-1, 1)))(negprojected2)\n",
        "        w1 = Lambda(lambda x: K.reshape(K.sum(x, axis=1), shape = (-1, 1)))(w)\n",
        "        \n",
        "        z1 = Lambda(lambda x: K.reshape(K.sum(x, axis=1),shape =  (-1, 1)))(z)\n",
        "        \n",
        "        multiply_layer = keras.layers.Multiply(name='y')\n",
        "        merged = multiply_layer([z1, w1])\n",
        "        \n",
        "    else:\n",
        "        w = Lambda(lambda x: K.reshape(K.sqrt(K.sum(x * x, axis=1)),shape =  (-1, 1)))(projected1) \n",
        "        \n",
        "        z = Lambda(lambda x: K.reshape(K.sqrt(K.sum(x * x, axis=1)), shape =(-1, 1)))(negprojected2)\n",
        "                  \n",
        "        merged = keras.layers.add([z, w], name='y')\n",
        "        \n",
        "   \n",
        "    model = Model(inputs=[inputA, inputB], outputs=merged)\n",
        "    model.compile(loss={'y' : lambda y, d: K.mean(y * tf.norm(d))}, optimizer=SimpleSGD())\n",
        "    return model\n",
        "\n",
        "\n",
        "def apply_embedding_transformation(embeddings, positive_seeds, negative_seeds,\n",
        "                                   n_epochs=5, n_dim=10, force_orthogonal=False,\n",
        "                                   plot=True, plot_points=50, plot_seeds=False,\n",
        "                                   **kwargs):\n",
        "    print (\"Preparing to learn embedding tranformation\")\n",
        "    dataset = DatasetMinibatchIterator(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
        "    model = get_model(embeddings.m.shape[1], n_dim, **kwargs)\n",
        "\n",
        "    print (\"Learning embedding transformation\")\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        dataset.shuffle()\n",
        "        loss = 0\n",
        "        \n",
        "        for i, X in enumerate(dataset):\n",
        "            loss += model.train_on_batch([X[ 'embeddings1'],X[ 'embeddings2']],X['y']) * X['y'].size\n",
        "           \n",
        "            Q, b = model.get_weights()\n",
        "            if force_orthogonal:\n",
        "                Q = orthogonalize(Q)\n",
        "            model.set_weights([Q, np.zeros_like(b)])\n",
        "    print(loss)\n",
        "    Q, b = model.get_weights()\n",
        "    new_mat = embeddings.m.dot(Q)[:,0:n_dim]\n",
        "    \n",
        "    if plot and n_dim == 2:\n",
        "#         plot_seeds = True\n",
        "\n",
        "        plot_words = list(positive_seeds.keys()) + list(negative_seeds.keys()) if plot_seeds else \\\n",
        "            [w for w in embeddings if w not in positive_seeds and w not in negative_seeds]\n",
        "#         plot_words = set(random.sample(plot_words, plot_points))\n",
        "        to_plot = {w: embeddings[w].dot(Q)[0:n_dim] for w in embeddings if w in plot_words}\n",
        "        \n",
        "\n",
        "        \n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for w, e in to_plot.items():\n",
        "            plt.text(e[0], e[1], w,\n",
        "                     bbox=dict(facecolor='green' if w in positive_seeds else 'red', alpha=0.1))\n",
        "        if len(list(to_plot.values()))!=0:\n",
        "\n",
        "          xmin, ymin = np.min(np.vstack(list(to_plot.values())),axis=0)\n",
        "          xmax, ymax = np.max(np.vstack(list(to_plot.values())),axis=0)\n",
        "          plt.xlim(xmin, xmax)\n",
        "          plt.ylim(ymin, ymax)\n",
        "          plt.show()\n",
        "    \n",
        "    return Embedding(new_mat, embeddings.iw, normalize=n_dim!=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-WdiBQjvzKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from multiprocessing import Pool\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "\n",
        "\"\"\"\n",
        "A set of methods for inducing polarity lexicons using word embeddings and seed words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def densify(embeddings, positive_seeds, negative_seeds, \n",
        "        transform_method=apply_embedding_transformation, **kwargs):\n",
        "    \"\"\"\n",
        "    Learns polarity scores via orthogonally-regularized projection to one-dimension\n",
        "    Adapted from: http://arxiv.org/pdf/1602.07572.pdf\n",
        "    \"\"\"\n",
        "    p_seeds = {word:1.0 for word in positive_seeds}\n",
        "    n_seeds = {word:1.0 for word in negative_seeds}\n",
        "    new_embeddings = embeddings\n",
        "    new_embeddings = apply_embedding_transformation(\n",
        "            embeddings, p_seeds, n_seeds, n_dim=1,  **kwargs)\n",
        "    polarities = {w:new_embeddings[w][0] for w in embeddings.iw}\n",
        "    \n",
        "    \n",
        "\n",
        "    return  polarities\n",
        "\n",
        "\n",
        "\n",
        "def random_walk(embeddings, positive_seeds, negative_seeds, beta=0.9, **kwargs):\n",
        "    \"\"\"\n",
        "    Learns polarity scores via random walks with teleporation to seed sets.\n",
        "    Main method used in paper. \n",
        "    \"\"\"\n",
        "    def run_random_walk(M, teleport, beta, **kwargs):\n",
        "        def update_seeds(r):\n",
        "            r += (1 - beta) * teleport / np.sum(teleport)\n",
        "        return run_iterative(M * beta, np.ones(M.shape[1]) / M.shape[1], update_seeds, **kwargs)\n",
        "\n",
        "    if not type(positive_seeds) is dict:\n",
        "        positive_seeds = {word:1.0 for word in positive_seeds}\n",
        "        negative_seeds = {word:1.0 for word in negative_seeds}\n",
        "    words = embeddings.iw\n",
        "    M = transition_matrix(embeddings, **kwargs)\n",
        "    rpos = run_random_walk(M, weighted_teleport_set(words, positive_seeds), beta, **kwargs)\n",
        "    rneg = run_random_walk(M, weighted_teleport_set(words, negative_seeds), beta, **kwargs)\n",
        "    return {w: rpos[i] / (rpos[i] + rneg[i]) for i, w in enumerate(words)}\n",
        "\n",
        "\n",
        "def label_propagate_probabilistic(embeddings, positive_seeds, negative_seeds, **kwargs):\n",
        "    \"\"\"\n",
        "    Learns polarity scores via standard label propagation from seed sets.\n",
        "    One walk per label. Scores normalized to probabilities. \n",
        "    \"\"\"\n",
        "    words = embeddings.iw\n",
        "    M = transition_matrix(embeddings, **kwargs)\n",
        "    pos, neg = teleport_set(words, positive_seeds), teleport_set(words, negative_seeds)\n",
        "    def update_seeds(r):\n",
        "        r[pos] = [1, 0]\n",
        "        r[neg] = [0, 1]\n",
        "        r /= np.sum(r, axis=1)[:, np.newaxis]\n",
        "    r = run_iterative(M, np.random.random((M.shape[0], 2)), update_seeds, **kwargs)\n",
        "    return {w: r[i][0] / (r[i][0] + r[i][1]) for i, w in enumerate(words)}\n",
        "\n",
        "\n",
        "def label_propagate_continuous(embeddings, positive_seeds, negative_seeds, **kwargs):\n",
        "    \"\"\"\n",
        "    Learns polarity scores via standard label propagation from seed sets.\n",
        "    One walk for both labels, continuous non-normalized scores.\n",
        "    \"\"\"\n",
        "    words = embeddings.iw\n",
        "    M = transition_matrix(embeddings, **kwargs)\n",
        "    pos, neg = teleport_set(words, positive_seeds), teleport_set(words, negative_seeds)\n",
        "    def update_seeds(r):\n",
        "        r[pos] = 1\n",
        "        r[neg] = -1\n",
        "    r = run_iterative(M, np.zeros(M.shape[0]), update_seeds, **kwargs)\n",
        "    return {w: r[i] for i, w in enumerate(words)}\n",
        "\n",
        "from  keras.utils.generic_utils import Progbar\n",
        "def logged_loop(iterable, n=None):\n",
        "    if n is None:\n",
        "        n = len(iterable)\n",
        "    step = max(1, n / 1000)\n",
        "    prog = Progbar(n)\n",
        "    for i, elem in enumerate(iterable):\n",
        "        if i % step == 0 or i == n - 1:\n",
        "            prog.update(i + 1)\n",
        "        yield elem\n",
        "        \n",
        "def graph_propagate(embeddings, positive_seeds, negative_seeds, **kwargs):\n",
        "    \"\"\"\n",
        "    Graph propagation method dapted from Velikovich, Leonid, et al. \"The viability of web-derived polarity lexicons.\"\n",
        "    http://www.aclweb.org/anthology/N10-1119\n",
        "    Should be used with arccos=True\n",
        "    \"\"\"\n",
        "    def run_graph_propagate(seeds, alpha_mat, trans_mat, T=1, **kwargs):\n",
        "        def get_rel_edges(ind_set):\n",
        "            rel_edges = set([])\n",
        "            for node in ind_set:\n",
        "                rel_edges = rel_edges.union(\n",
        "                        [(node, other) for other in trans_mat[node,:].nonzero()[1]])\n",
        "            return rel_edges\n",
        "\n",
        "        for seed in seeds:\n",
        "            F = set([seed])\n",
        "            for t in range(T):\n",
        "                for edge in get_rel_edges(F):\n",
        "                    alpha_mat[seed, edge[1]] = max(\n",
        "                            alpha_mat[seed, edge[1]], \n",
        "                            alpha_mat[seed, edge[0]] * trans_mat[edge[0], edge[1]])\n",
        "                    F.add(edge[1])\n",
        "        return alpha_mat\n",
        "\n",
        "    M = similarity_matrix(embeddings, **kwargs)\n",
        "    M = (M + M.T)/2\n",
        "    print (\"Getting positive scores..\")\n",
        "    pos_alpha = M.copy()\n",
        "    neg_alpha = M.copy()\n",
        "    M = csr_matrix(M)\n",
        "    pos_alpha = run_graph_propagate([embeddings.wi[seed] for seed in positive_seeds],\n",
        "            pos_alpha, M, **kwargs)\n",
        "    pos_alpha = pos_alpha + pos_alpha.T\n",
        "    print (\"Getting negative scores..\")\n",
        "    neg_alpha = run_graph_propagate([embeddings.wi[seed] for seed in negative_seeds],\n",
        "            neg_alpha, M, **kwargs)\n",
        "    neg_alpha = neg_alpha + neg_alpha.T\n",
        "    print (\"Computing final scores...\")\n",
        "    polarities = {}\n",
        "    index = embeddings.wi\n",
        "    pos_pols = {w:1.0 for w in positive_seeds}\n",
        "    for w in negative_seeds:\n",
        "        pos_pols[w] = 0.0\n",
        "    neg_pols = {w:1.0 for w in negative_seeds}\n",
        "    for w in positive_seeds:\n",
        "        neg_pols[w] = 0.0\n",
        "    for w in logged_loop(index):\n",
        "        if w not in positive_seeds and w not in negative_seeds:\n",
        "            pos_pols[w] = sum(pos_alpha[index[w], index[seed]] for seed in positive_seeds if seed in index) \n",
        "            neg_pols[w] = sum(neg_alpha[index[w], index[seed]] for seed in negative_seeds if seed in index)\n",
        "    beta = np.sum(list(pos_pols.values())) / np.sum(list(neg_pols.values()))\n",
        "    for w in index:\n",
        "        polarities[w] = pos_pols[w] - beta * neg_pols[w]\n",
        "    return polarities\n",
        "\n",
        "\n",
        "### HELPER METHODS #####\n",
        "\n",
        "def teleport_set(words, seeds):\n",
        "    return [i for i, w in enumerate(words) if w in seeds]\n",
        "\n",
        "def weighted_teleport_set(words, seed_weights):\n",
        "    return np.array([seed_weights[word] if word in seed_weights else 0.0 for word in words])\n",
        "\n",
        "def run_iterative(M, r, update_seeds, max_iter=50, epsilon=1e-6, **kwargs):\n",
        "    for i in range(max_iter):\n",
        "        last_r = np.array(r)\n",
        "        r = np.dot(M, r)\n",
        "        update_seeds(r)\n",
        "        if np.abs(r - last_r).sum() < epsilon:\n",
        "            break\n",
        "    return r\n",
        "\n",
        "### META METHODS ###\n",
        "\n",
        "def _bootstrap_func(embeddings, positive_seeds, negative_seeds, boot_size, score_method, seed, **kwargs):\n",
        "    np.random.seed(seed)\n",
        "    pos_seeds = np.random.choice(positive_seeds, boot_size)\n",
        "    neg_seeds = np.random.choice(negative_seeds, boot_size)\n",
        "    polarities = score_method(embeddings, pos_seeds, neg_seeds, **kwargs)\n",
        "    return {word:score for word, score in polarities.items() if\n",
        "            not word in positive_seeds and not word in negative_seeds}\n",
        "\n",
        "def bootstrap(embeddings, positive_seeds, negative_seeds, num_boots=10, score_method=random_walk,\n",
        "        boot_size=7, return_all=False, n_procs=15, **kwargs):\n",
        "    pool = Pool(n_procs)\n",
        "    map_func = functools.partial(_bootstrap_func, embeddings, positive_seeds, negative_seeds,\n",
        "            boot_size, score_method, **kwargs)\n",
        "    polarities_list = pool.map(map_func, range(num_boots))\n",
        "    if return_all:\n",
        "        return polarities_list\n",
        "    else:\n",
        "        polarities = {}\n",
        "        for word in polarities_list[0]:\n",
        "            polarities[word] = np.mean([polarities_list[i][word] for i in range(num_boots)])\n",
        "        return polarities\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epp-xa0ZwFiJ",
        "colab_type": "code",
        "outputId": "cb56e9fb-77a6-4eb4-e5f3-7dd5ce889359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def lines(fname):\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            yield line\n",
        "\n",
        "def create_representation(rep_type, path, *args, **kwargs):\n",
        "    if rep_type == 'Explicit':\n",
        "        return Explicit.load(path, *args, **kwargs)\n",
        "    elif rep_type == 'SVD':\n",
        "        return SVDEmbedding(path, *args, **kwargs)\n",
        "    elif rep_type == 'GIGA':\n",
        "        return GigaEmbedding(path, *args, **kwargs)\n",
        "    else:\n",
        "        return Embedding.load(path, *args, **kwargs)\n",
        "\n",
        "common_embed = create_representation(\"GIGA\", embeddings, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:117: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hobi7Aa4fo7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ekNAhejz-ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
        "        method=densify, **kwargs):\n",
        "    if transform_embeddings:\n",
        "        print (\"Transforming embeddings...\")\n",
        "        embeddings = apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
        "    if post_densify:\n",
        "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
        "        top_pos = [word for word in \n",
        "                sorted(polarities, key = lambda w : -polarities[w])[:5]]#150\n",
        "        top_neg = [word for word in \n",
        "                sorted(polarities, key = lambda w : polarities[w])[:5]]#150\n",
        "        top_pos.extend(positive_seeds)\n",
        "        top_neg.extend(negative_seeds)\n",
        "        return densify(embeddings, top_pos, top_neg)\n",
        "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
        "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
        "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-yISUxx4obk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "positive_seeds, negative_seeds = finance_seeds()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78tvVZxrZeGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "input_str = \"Tunisia Industrial Output Shrinks for 7th Straight Month.\"\n",
        "input_str = re.sub(r'\\d+', '', input_str)\n",
        "\n",
        "# print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIpdjYZhzk6o",
        "colab_type": "code",
        "outputId": "631534da-90d7-470b-aaeb-079a63294ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "eval_words = ['shrink','fall', 'failure','shock','low','hits','good','best','profitable','best','worst','upwarding']\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "example_sent = \"Tunisia Industrial Output Shrinks for 7th Straight Month\"\n",
        "\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
        "# eval_words = ['shrink']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBWIYKpdVkiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# too much ram\n",
        "'''\n",
        "label_propagate_probabilistic\n",
        "'''\n",
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=True,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=50,\n",
        "        force_orthogonal=False,\n",
        "        batch_size=100,\n",
        "        cosine=False,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "polarities = run_method(positive_seeds, negative_seeds, \n",
        "            common_embed,\n",
        "            method=label_propagate_continuous, #label_propagate_probabilistic\n",
        "            #method=bootstrap, \n",
        "            beta=0.99, nn=5,\n",
        "#             transform_embeddings=True,\n",
        "\n",
        "            **DEFAULT_ARGUMENTS)\n",
        "from operator import itemgetter\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(polarities.keys())))(polarities)\n",
        "myvalues,list(set(eval_words) & set(polarities.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddXMFtihWuYE",
        "colab_type": "code",
        "outputId": "5e0431a3-4a3b-4d6c-ca8f-866932b5d040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "'''\n",
        "graph_propagate\n",
        "'''\n",
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=True,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=100,\n",
        "        force_orthogonal=True,\n",
        "        batch_size=100,\n",
        "        cosine=False,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "polarities = run_method(positive_seeds, negative_seeds, \n",
        "                        common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "#                         score_method=bootstrap,\n",
        "                        method=graph_propagate,\n",
        "                        T=10,\n",
        "                        transform_embeddings=True,\n",
        "                        **DEFAULT_ARGUMENTS)\n",
        "from operator import itemgetter\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(polarities.keys())))(polarities)\n",
        "dict(zip(list(set(eval_words) & set(polarities.keys())),myvalues))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transforming embeddings...\n",
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  3.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-959.7970199584961\n",
            "Getting positive scores..\n",
            "Getting negative scores..\n",
            "Computing final scores...\n",
            "29/29 [==============================] - 0s 49us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best': 2.8911447403992643,\n",
              " 'failure': -0.8756833698491466,\n",
              " 'fall': -0.15372589662434777,\n",
              " 'good': 2.913776825427723,\n",
              " 'hits': -1.2046365206698262,\n",
              " 'low': 2.3917099272276605,\n",
              " 'profitable': 1.777632448114801,\n",
              " 'shock': -5.100133118168005,\n",
              " 'shrink': -0.5905424908226706,\n",
              " 'worst': -2.2927088465439827}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbDC0Zt3V6aP",
        "colab_type": "code",
        "outputId": "1dc5131d-fcb1-4279-c3fa-4f3575d9fe6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "'''\n",
        "random walk\n",
        "'''\n",
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=True,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=50,\n",
        "        force_orthogonal=True,\n",
        "        batch_size=100,\n",
        "        cosine=False,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "nn=10\n",
        "beta=0.1\n",
        "polarities = run_method(positive_seeds, negative_seeds, \n",
        "                    common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                    score_method=bootstrap, \n",
        "                    method=random_walk, \n",
        "                    nn=nn, beta=beta,\n",
        "                    transform_embeddings=True,\n",
        "                    **DEFAULT_ARGUMENTS)\n",
        "\n",
        "from operator import itemgetter\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(polarities.keys())))(polarities)\n",
        "dict(zip(list(set(eval_words) & set(polarities.keys())),myvalues))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transforming embeddings...\n",
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:02<00:00,  2.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-959.796838760376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best': 0.8763161580160217,\n",
              " 'failure': 0.002433381754637584,\n",
              " 'fall': 0.3797587947562773,\n",
              " 'good': 0.7879682680414204,\n",
              " 'hits': 0.25277628664878626,\n",
              " 'low': 0.7147992797703622,\n",
              " 'profitable': 0.868881296745753,\n",
              " 'shock': 0.18137989348575925,\n",
              " 'shrink': 0.38013812533565233,\n",
              " 'worst': 0.040143569649967056}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8uO1qUUMRti",
        "colab_type": "code",
        "outputId": "041d1b18-d187-46e7-d701-8a0e753f765d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=True,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=50,\n",
        "        force_orthogonal=True,\n",
        "        batch_size=100,\n",
        "        cosine=True,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "polarities = run_method(positive_seeds, negative_seeds, \n",
        "                          common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                          method=densify, \n",
        "                        post_densify = True,\n",
        "                          lr=0.01, regularization_strength=0.01,\n",
        "\n",
        "                          **DEFAULT_ARGUMENTS)\n",
        "from operator import itemgetter\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(polarities.keys())))(polarities)\n",
        "dict(zip(list(set(eval_words) & set(polarities.keys())),myvalues))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:12<00:00,  4.09it/s]\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-73.84199440479279\n",
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:08<00:00,  1.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-862.3596575260162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best': -3.3844862,\n",
              " 'failure': 0.06777992,\n",
              " 'fall': -1.218575,\n",
              " 'good': -4.6377993,\n",
              " 'hits': -0.7553856,\n",
              " 'low': -1.9848655,\n",
              " 'profitable': -5.490222,\n",
              " 'shock': -0.032394607,\n",
              " 'shrink': -1.7407255,\n",
              " 'worst': -1.2267815}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yApVXC8HNnu5",
        "colab_type": "code",
        "outputId": "71f7b7cd-f319-4aab-911f-a1e03a7219cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "data = np.array(list(polarities.values())).reshape(-1, 1)\n",
        "scaler = MinMaxScaler()\n",
        "v = scaler.fit_transform(data)\n",
        "\n",
        "a = dict(zip(polarities.keys(), v.flatten()))\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(a.keys())))(a)\n",
        "dict(zip(list(set(eval_words) & set(a.keys())),myvalues))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best': 0.48350433,\n",
              " 'failure': 0.9802322,\n",
              " 'fall': 0.7951455,\n",
              " 'good': 0.30317193,\n",
              " 'hits': 0.8617913,\n",
              " 'low': 0.6848881,\n",
              " 'profitable': 0.18052143,\n",
              " 'shock': 0.9658186,\n",
              " 'shrink': 0.7200161,\n",
              " 'worst': 0.79396474}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3uC9wlPYh6",
        "colab_type": "code",
        "outputId": "0fa693f6-1819-41ac-83c6-7c1d7a929d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "d=dict(zip(list(set(eval_words) & set(a.keys())),myvalues))\n",
        "from toolz.dicttoolz import valmap\n",
        "valmap(lambda x: 1-x, d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best': 0.21649567484855647,\n",
              " 'failure': -0.2802321791648865,\n",
              " 'fall': -0.09514551162719731,\n",
              " 'good': 0.3968280673027038,\n",
              " 'hits': -0.1617913126945496,\n",
              " 'low': 0.015111875534057573,\n",
              " 'profitable': 0.5194785714149475,\n",
              " 'shock': -0.26581858396530156,\n",
              " 'shrink': -0.020016121864318892,\n",
              " 'worst': -0.09396474361419682}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svZdDZcV4qn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "densify\n",
        "'''\n",
        "lr =  [0.01]\n",
        "reg = [0.001, 0.003, 0.005, 0.007, 0.009]\n",
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=False,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=50,\n",
        "        force_orthogonal=True,\n",
        "        batch_size=1,\n",
        "        cosine=False,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "for lr_ in lr:\n",
        "  for reg_ in reg : \n",
        "    polarities = run_method(positive_seeds, negative_seeds, \n",
        "                          common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                          method=densify, \n",
        "                          lr=lr_, regularization_strength=reg_,\n",
        "\n",
        "                          **DEFAULT_ARGUMENTS)\n",
        "\n",
        "    print(polarities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD3v1mmb4qqc",
        "colab_type": "code",
        "outputId": "242b7362-2bc1-46f5-e7aa-7cf0cbc0f846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5202
        }
      },
      "source": [
        "# with more seed words ( bullish, bearish ...)\n",
        "lr =  [0.001, 0.01, 0.1, 0.5]\n",
        "reg = [0.001, 0.003, 0.005, 0.007, 0.009, 0.01, 0.1, 0.5]\n",
        "\n",
        "DEFAULT_ARGUMENTS = dict(\n",
        "        # for iterative graph algorithms\n",
        "        similarity_power=1,\n",
        "        arccos=False,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        sym=True,\n",
        "\n",
        "        # for learning embeddings transformation\n",
        "        n_epochs=50,\n",
        "        force_orthogonal=True,\n",
        "        batch_size=1,\n",
        "        cosine=False,\n",
        "\n",
        "        ## bootstrap\n",
        "        num_boots=1,\n",
        "        n_procs=1,\n",
        ")\n",
        "for lr_ in lr:\n",
        "  for reg_ in reg : \n",
        "    polarities = run_method(positive_seeds, negative_seeds, \n",
        "                          common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                          method=densify, \n",
        "                          lr=lr_, regularization_strength=reg_,\n",
        "\n",
        "                          **DEFAULT_ARGUMENTS)\n",
        "\n",
        "    print(polarities)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:18<15:08, 18.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:30<13:08, 16.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:41<11:40, 14.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:52<10:35, 13.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [01:03<09:47, 13.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [01:15<09:08, 12.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [01:26<08:38, 12.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [01:37<08:15, 11.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [01:48<07:55, 11.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [01:59<07:39, 11.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [02:10<07:24, 11.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [02:21<07:08, 11.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [02:33<06:55, 11.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [02:44<06:42, 11.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [02:55<06:29, 11.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [03:06<06:20, 11.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [03:17<06:09, 11.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [03:28<05:56, 11.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [03:39<05:44, 11.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [03:50<05:32, 11.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [04:01<05:21, 11.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [04:12<05:10, 11.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [04:23<04:59, 11.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [04:35<04:49, 11.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [04:46<04:37, 11.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [04:57<04:26, 11.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [05:08<04:14, 11.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [05:19<04:02, 11.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [05:30<03:50, 10.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [05:41<03:39, 10.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [05:52<03:29, 11.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [06:03<03:18, 11.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [06:14<03:06, 11.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [06:25<02:55, 10.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [06:35<02:44, 10.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [06:46<02:33, 10.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [06:57<02:22, 10.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [07:08<02:11, 10.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c8d87664351a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                           \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_strength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                           **DEFAULT_ARGUMENTS)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-3c447601708e>\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(positive_seeds, negative_seeds, embeddings, transform_embeddings, post_densify, method, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpositive_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_seeds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnegative_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_seeds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_seeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_seeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-7e4ca7e3064d>\u001b[0m in \u001b[0;36mdensify\u001b[0;34m(embeddings, positive_seeds, negative_seeds, transform_method, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnew_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     new_embeddings = apply_embedding_transformation(\n\u001b[0;32m---> 32\u001b[0;31m             embeddings, p_seeds, n_seeds, n_dim=2,  **kwargs)\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpolarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miw\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-e424cffd5d54>\u001b[0m in \u001b[0;36mapply_embedding_transformation\u001b[0;34m(embeddings, positive_seeds, negative_seeds, n_epochs, n_dim, force_orthogonal, plot, plot_points, plot_seeds, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce_orthogonal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morthogonalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-e424cffd5d54>\u001b[0m in \u001b[0;36morthogonalize\u001b[0;34m(Q)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0morthogonalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_QYdfeeClWe",
        "colab_type": "code",
        "outputId": "6c2570f5-faaf-4a50-ba00-5e965bff979c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "def apply_embedding_transformation_grid(embeddings, positive_seeds, negative_seeds,\n",
        "                                   n_epochs=5, n_dim=10, force_orthogonal=False,\n",
        "                                   plot=True, plot_points=50, plot_seeds=False,\n",
        "                                   **kwargs):\n",
        "   \n",
        "    dataset = DatasetMinibatchIterator(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
        "    model = get_model(embeddings.m.shape[1], n_dim, **kwargs)\n",
        "\n",
        "    \n",
        "\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        dataset.shuffle()\n",
        "        loss = 0\n",
        "        \n",
        "        for i, X in enumerate(dataset):\n",
        "            loss += model.train_on_batch([X[ 'embeddings1'],X[ 'embeddings2']],X['y']) * X['y'].size\n",
        "           \n",
        "            Q, b = model.get_weights()\n",
        "            if force_orthogonal:\n",
        "                Q = orthogonalize(Q)\n",
        "            model.set_weights([Q, np.zeros_like(b)])\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "def densify_grid(embeddings, positive_seeds, negative_seeds, \n",
        "        transform_method=apply_embedding_transformation, **kwargs):\n",
        "\n",
        "    p_seeds = {word:1.0 for word in positive_seeds}\n",
        "    n_seeds = {word:1.0 for word in negative_seeds}\n",
        "    new_embeddings = embeddings\n",
        "    return apply_embedding_transformation_grid(\n",
        "            embeddings, p_seeds, n_seeds, n_dim=2,  **kwargs)\n",
        "  \n",
        "def run_method_grid(positive_seeds, negative_seeds, embeddings, transform_embeddings=False,\n",
        "        method=densify, **kwargs):\n",
        "    print('new')\n",
        "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
        "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
        "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def f(x,y):\n",
        "  return - run_method_grid(positive_seeds, negative_seeds, \n",
        "                            common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                            method=densify_grid, \n",
        "                            lr=x, regularization_strength=y,\n",
        "\n",
        "                            similarity_power=1,\n",
        "                            arccos=False,\n",
        "                            max_iter=50,\n",
        "                            epsilon=1e-6,\n",
        "                            sym=True,\n",
        "\n",
        "                            # for learning embeddings transformation\n",
        "                            n_epochs=50,\n",
        "                            force_orthogonal=True,\n",
        "                            batch_size=1,\n",
        "                            cosine=False,\n",
        "\n",
        "                            ## bootstrap\n",
        "                            num_boots=1,\n",
        "                            n_procs=1)\n",
        "\n",
        "\n",
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'x': (0.001, 0.5), 'y': (0.001, 0.5)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=f,\n",
        "    pbounds=pbounds,\n",
        "    random_state=1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUZ51pK2E2Z0",
        "colab_type": "code",
        "outputId": "192a59a0-9add-48eb-cee1-b1aacd20dbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1597
        }
      },
      "source": [
        "optimizer.maximize(\n",
        "    init_points=2,\n",
        "    n_iter=20,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |     x     |     y     |\n",
            "-------------------------------------------------\n",
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:25<00:00,  6.29s/it]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 5.819   \u001b[0m | \u001b[0m 0.2091  \u001b[0m | \u001b[0m 0.3604  \u001b[0m |\n",
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:08<00:00,  6.04s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 5.505   \u001b[0m | \u001b[0m 0.001057\u001b[0m | \u001b[0m 0.1519  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:00<00:00,  5.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 5.077   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:58<00:00,  5.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 3.724   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:58<00:00,  5.86s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[95m 5       \u001b[0m | \u001b[95m 12.67   \u001b[0m | \u001b[95m 0.02711 \u001b[0m | \u001b[95m 0.25    \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:02<00:00,  5.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 8.688   \u001b[0m | \u001b[0m 0.4429  \u001b[0m | \u001b[0m 0.4099  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:59<00:00,  5.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6019  \u001b[0m | \u001b[0m 0.00761 \u001b[0m | \u001b[0m 0.3799  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:58<00:00,  5.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 3.743   \u001b[0m | \u001b[0m 0.4356  \u001b[0m | \u001b[0m 0.2406  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:59<00:00,  5.91s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 3.155   \u001b[0m | \u001b[0m 0.2091  \u001b[0m | \u001b[0m 0.3606  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:59<00:00,  5.89s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 4.719   \u001b[0m | \u001b[0m 0.4779  \u001b[0m | \u001b[0m 0.4186  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:00<00:00,  5.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 6.932   \u001b[0m | \u001b[0m 0.2288  \u001b[0m | \u001b[0m 0.4808  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:06<00:00,  6.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 9.404   \u001b[0m | \u001b[0m 0.006549\u001b[0m | \u001b[0m 0.377   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:02<00:00,  5.98s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 9.954   \u001b[0m | \u001b[0m 0.02708 \u001b[0m | \u001b[0m 0.2497  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:55<00:00,  5.79s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 9.639   \u001b[0m | \u001b[0m 0.006675\u001b[0m | \u001b[0m 0.3773  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:57<00:00,  5.84s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 15      \u001b[0m | \u001b[0m 6.342   \u001b[0m | \u001b[0m 0.4434  \u001b[0m | \u001b[0m 0.4099  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:56<00:00,  5.79s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 16      \u001b[0m | \u001b[0m 1.475   \u001b[0m | \u001b[0m 0.006587\u001b[0m | \u001b[0m 0.3766  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:57<00:00,  5.81s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.7483  \u001b[0m | \u001b[0m 0.006663\u001b[0m | \u001b[0m 0.3775  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [05:00<00:00,  5.86s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 18      \u001b[0m | \u001b[0m 5.069   \u001b[0m | \u001b[0m 0.2288  \u001b[0m | \u001b[0m 0.4805  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:57<00:00,  5.81s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 19      \u001b[0m | \u001b[0m 11.84   \u001b[0m | \u001b[0m 0.2218  \u001b[0m | \u001b[0m 0.4267  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:58<00:00,  5.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 20      \u001b[0m | \u001b[0m 5.941   \u001b[0m | \u001b[0m 0.4431  \u001b[0m | \u001b[0m 0.4098  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:55<00:00,  5.78s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 21      \u001b[0m | \u001b[0m 4.071   \u001b[0m | \u001b[0m 0.02667 \u001b[0m | \u001b[0m 0.2504  \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [04:57<00:00,  5.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 22      \u001b[0m | \u001b[0m 11.86   \u001b[0m | \u001b[0m 0.443   \u001b[0m | \u001b[0m 0.4101  \u001b[0m |\n",
            "=================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJDxh2A1UPr",
        "colab_type": "code",
        "outputId": "a1faf725-4de7-45a2-db66-ffcc42cb2e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "optimizer.max"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'params': {'x': 0.027113930472524774, 'y': 0.25001579388326517},\n",
              " 'target': 12.674488961696625}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZLtV9p9SnDy",
        "colab_type": "code",
        "outputId": "00b8f9f9-594d-42d6-89e7-cb8cdaaf4a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "polarities = run_method(positive_seeds, negative_seeds, \n",
        "                          common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
        "                          method=densify, \n",
        "                          lr=optimizer.max['params']['x'], regularization_strength=optimizer.max['params']['y'],\n",
        "\n",
        "                          **DEFAULT_ARGUMENTS)\n",
        "from operator import itemgetter\n",
        "myvalues = itemgetter(*list(set(eval_words) & set(polarities.keys())))(polarities)\n",
        "# d2 = valmap(lambda x: -x, polarities)\n",
        "myvalues,list(set(eval_words) & set(polarities.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"identity\", kernel_constraint=<__main__....)`\n",
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing to learn embedding tranformation\n",
            "Learning embedding transformation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [07:45<00:00,  9.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzXHwt_nImJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvRadNU5wQxo",
        "colab_type": "text"
      },
      "source": [
        "#**Word2vec + wordnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iWb6xx9F6ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from gensim import matutils\n",
        "from six import string_types\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR0RmbdIx27C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findWordNet( key):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "            ----------\n",
        "            key : str\n",
        "                the word you want to find in zh_wordnet.\n",
        "        Returns\n",
        "            ----------\n",
        "            list\n",
        "                a list in wordnet format.\n",
        "        \"\"\"\n",
        "        synonyms = []\n",
        "\n",
        "        for syn in wn.synsets(key):\n",
        "            for l in syn.lemmas():\n",
        "                synonyms.append(syn)\n",
        "        return(synonyms)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfcXOtxYGXg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim, logging\n",
        "import os\n",
        "class MySentences(object):\n",
        "    def __init__(self):\n",
        "      pass\n",
        " \n",
        "    def __iter__(self):\n",
        "        for fname in drive.ListFile({'q': \"'1HrGCbwzvK2WPE559f9gWgzgbcBhTXRjD' in parents\"}).GetList():\n",
        "            fp = open(f['title'])\n",
        "            for line in fp:\n",
        "                yield line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePhY8wcmxU7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk3i-veQxBnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = MySentences() # a memory-friendly iterator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4yAleV7UyRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in sentences:\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEp9w6LgxsKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Label_index=positive_seeds + negative_seeds\n",
        "Label_dict=dict(zip(positive_seeds,[1]*len(positive_seeds)))\n",
        "Label_dict.update(dict(zip(negative_seeds,[-1]*len(negative_seeds))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8TQlaFxBpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences)\n",
        "model = gensim.models.Word2Vec(sentences, min_count=20)\n",
        "len_vector=model.trainables.layer1_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuN3TbVBxBsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "stopWords = set(stopwords.words('english'))\n",
        "words = word_tokenize(data)\n",
        "wordsFiltered = []\n",
        "\n",
        "for w in words:\n",
        "    if w not in stopWords:\n",
        "        wordsFiltered.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuBX2-1Nxx55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Label_vec=np.empty((len(Label_dict),len_vector))\n",
        "Label_vec_u=np.empty((len(Label_dict),len_vector))\n",
        "for i in range(len(Label_index)):\n",
        "    try:\n",
        "       \n",
        "        Label_vec[i,:]=model.wv.__getitem__(Label_index[i])\n",
        "        Label_vec_u[i,:]=matutils.unitvec(model.wv.__getitem__(Label_index[i]))\n",
        "    except:\n",
        "        Label_vec[i,:]=np.zeros((1,len_vector)) \n",
        "        Label_vec_u[i,:]=np.zeros((1,len_vector))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnCZFVVwx_rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Label_wn=dict()\n",
        "for key in Label_dict.keys():\n",
        "    ll=findWordNet(key)\n",
        "    Label_wn[key]=list()\n",
        "    for l in ll:\n",
        "        Label_wn[key].append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHb_BGxDx_uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity_label( words, normalization=True):\n",
        "        \"\"\"\n",
        "        you can calculate more than one word at the same time.\n",
        "        \"\"\"\n",
        "        if model==None:\n",
        "            raise Exception('no model.')\n",
        "        if isinstance(words, string_types):\n",
        "            words=[words]\n",
        "        vectors=np.transpose(model.wv.__getitem__(words))\n",
        "        if normalization:\n",
        "            unit_vector=unitvec(vectors,ax=0) \n",
        "            dists=np.dot(Label_vec_u, unit_vector)\n",
        "        else:\n",
        "            dists=np.dot(Label_vec, vectors)\n",
        "        return dists\n",
        "    \n",
        "    \n",
        "def topn_similarity_label( words, topn=10, normalization=True):\n",
        "        if model==None:\n",
        "            raise Exception('no model.')\n",
        "        if isinstance(words, string_types):\n",
        "            words=[words]\n",
        "    \n",
        "        vectors=np.transpose(model.wv.__getitem__(words))\n",
        "        \n",
        "        if normalization:\n",
        "            unit_vector=unitvec(vectors,ax=0)\n",
        "            dists=np.dot(Label_vec_u, unit_vector)\n",
        "        else:\n",
        "            dists=np.dot(Label_vec, vectors)\n",
        "        \n",
        "        topwords=[]\n",
        "        topsims=np.empty((topn,len(words)))\n",
        "        best = np.argsort(dists, axis=0)\n",
        "        for i in range(topn):\n",
        "            topword=[]\n",
        "            for j in range(len(words)):\n",
        "                topword.append(Label_index[best[-i-1][j]])\n",
        "                topsims[i][j]=dists[best[-i-1][j]][j]\n",
        "            topwords.append(topword)\n",
        "        result=[(topwords[i], topsims[i]) for i in range(topn)]\n",
        "        return result\n",
        "        \"\"\" print this result by:\n",
        "\n",
        "            | for iword,isim in result:  |\n",
        "            |     print(iword, isim)     |\n",
        "            or\n",
        "            | for iword, isim in b:                               |\n",
        "            |     for i in range(len(b[0])):                      |\n",
        "            |         print(\"%s:%f\\t\" %(iword[i],isim[i]),end=\"\") |\n",
        "            |     print(\"\")                                       |\n",
        "                \n",
        "        \"\"\"\n",
        "                \n",
        "def synonym_label( word, calc='all' ,calc_k=5):\n",
        "        \"\"\"\n",
        "        you can only calculate one word for one time.\n",
        "        \"\"\"\n",
        "        ww=list()\n",
        "        for w in findWordNet(word):\n",
        "            ww.append(w)\n",
        "        if (len(ww)==0):\n",
        "            #return 0\n",
        "            raise Exception('cannot be found.')\n",
        "        else:\n",
        "            similarities=[0]*len(Label_index)\n",
        "            if calc=='all': \n",
        "                for i in range(len(Label_index)):\n",
        "                    count=0\n",
        "                    for w in ww:\n",
        "                        for l in Label_wn[Label_index[i]]:\n",
        "                            \n",
        "                            sim=w.path_similarity(l)\n",
        "                            if(sim!=None):\n",
        "                                similarities[i]+=sim\n",
        "                            else:\n",
        "                                count+=1\n",
        "                    try:\n",
        "                        similarities[i]/=(len(ww)*len(Label_wn[Label_index[i]])-count) \n",
        "                    except:\n",
        "                        similarities[i]=0\n",
        "                        \n",
        "            elif calc=='calc_k': \n",
        "                for i in range(len(Label_index)):\n",
        "                    count=0\n",
        "                    simlist=[]\n",
        "                    for w in ww:\n",
        "                        for l in Label_wn[Label_index[i]]:\n",
        "                            sim=w.path_similarity(l)\n",
        "                            if(sim!=None):\n",
        "                                simlist.append(sim)\n",
        "                                count+=1\n",
        "                    if count<=calc_k:\n",
        "                        similarities[i]=np.mean(simlist)\n",
        "                    else:\n",
        "                        simlist=sorted(simlist,reverse=True)\n",
        "                        similarities[i]=simlist[:calc_k-1]/calc_k \n",
        "        return np.array(similarities)\n",
        "\n",
        "\n",
        "def topn_synonym_label( word, topn=10, calc='all', calc_k=5):\n",
        "        ww=list()\n",
        "        for w in findWordNet(word):\n",
        "            ww.append(w)\n",
        "        if (len(ww)==0):\n",
        "            return 0\n",
        "        else:\n",
        "            similarities=[0]*len(Label_index)\n",
        "            if calc=='all': \n",
        "                for i in range(len(Label_index)):\n",
        "                    count=0\n",
        "                    for w in ww:\n",
        "                        for l in Label_wn[Label_index[i]]:\n",
        "                            sim=w.path_similarity(l)\n",
        "                            if(sim!=None):\n",
        "                                similarities[i]+=sim\n",
        "                            else:\n",
        "                                count+=1\n",
        "                    try:\n",
        "                        similarities[i]/=(len(ww)*len(Label_wn[Label_index[i]])-count) \n",
        "                    except:\n",
        "                        similarities[i]=0\n",
        "                        \n",
        "            elif calc=='calc_k':\n",
        "                for i in range(len(Label_index)):\n",
        "                    count=0\n",
        "                    simlist=[]\n",
        "                    for w in ww:\n",
        "                        for l in Label_wn[Label_index[i]]:\n",
        "                            sim=w.path_similarity(l)\n",
        "                            if(sim!=None):\n",
        "                                simlist.append(sim)\n",
        "                                count+=1\n",
        "                    if count<=calc_k:\n",
        "                        similarities[i]=np.mean(simlist)\n",
        "                    else:\n",
        "                        simlist=sorted(simlist,reverse=True)\n",
        "                        similarities[i]=simlist[:calc_k-1]/calc_k \n",
        "                        \n",
        "        best=matutils.argsort(similarities, topn = topn, reverse=True)\n",
        "        result = [(Label_index[sim], float(similarities[sim])) for sim in best]\n",
        "        return result\n",
        "      \n",
        "def nlp_vector( words):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "            ----------\n",
        "            words : list of str/str \n",
        "                wordbag\n",
        "        Returns\n",
        "            ----------\n",
        "            ndarray(float)\n",
        "                the corresponding vectors of words in wordbag.\n",
        "                a vector contains the similarities calculated by word2vec and wordnet.\n",
        "        \"\"\"\n",
        "        if isinstance(words, string_types):\n",
        "            synonym=synonym_label(words)\n",
        "            similarity=similarity_label(words)\n",
        "        else:\n",
        "            synonym=np.empty((len(Label_index),len(words)))\n",
        "            similarity=np.empty((len(Label_index),len(words)))\n",
        "            for i in range(len(words)):\n",
        "                try:\n",
        "                    synonym[:,i]=synonym_label(words[i])\n",
        "                except:\n",
        "                    synonym[:,i]=np.zeros((len(Label_index),1))[:,0]\n",
        "                try:    \n",
        "                    similarity[:,i]=similarity_label(words[i])[:,0]\n",
        "                except:\n",
        "                    similarity[:,i]=np.zeros((len(Label_index),1))[:,0]\n",
        "        vector=np.concatenate((similarity, synonym.reshape(similarity.shape)))\n",
        "        return vector\n",
        "    \n",
        "    \n",
        "\n",
        "#---------------------------------math----------------------------------------\n",
        "     \n",
        "def unitvec(vector, ax=1):\n",
        "    v=vector*vector\n",
        "    if len(vector.shape)==1:\n",
        "        sqrtv=np.sqrt(np.sum(v))\n",
        "    elif len(vector.shape)==2:\n",
        "        sqrtv=np.sqrt([np.sum(v, axis=ax)])\n",
        "    else:\n",
        "        raise Exception('It\\'s too large.')\n",
        "    if ax==1:\n",
        "        result=np.divide(vector,sqrtv.T)\n",
        "    elif ax==0:\n",
        "        result=np.divide(vector,sqrtv)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-CUh0Fkx_wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "synonym_label('gain')\n",
        "topn_similarity_label('gain')\n",
        "topn_synonym_label('gain')\n",
        "similarity_label('gain')\n",
        "nlp_vector(['gain','the']).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdBWtsL4IYfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0mWzX5_IYjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1SooKSw8M4ACbznKjnNrYvJ5wxuqJ-YCk' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n",
        "\n",
        "\n",
        "with open(fname, 'r') as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}