{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = r\"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        #self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.word2id = {item:i for i, item in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        if type(w)==str:\n",
    "            similar = [self.score(self.word2vec[w], self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        else:\n",
    "            similar = [self.score(w, self.word2vec[self.id2word[i]]) for i in range(len(self.word2id))]\n",
    "        closest = np.argsort(similar)[::-1][:K]\n",
    "        results = [self.id2word[i] for i in closest]\n",
    "        return results\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if type(w1)==str:\n",
    "            emb1 = self.word2vec[w1]\n",
    "            emb2 = self.word2vec[w2]\n",
    "        else:\n",
    "            emb1 = w1\n",
    "            emb2 = w2\n",
    "        \n",
    "        score = np.dot(emb1, emb2)  / (np.linalg.norm(emb1)*np.linalg.norm(emb2))\n",
    "        return score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'pooches', 'Dogs', 'doggies']\n",
      "['paris', 'france', 'Paris', 'parisian', 'london']\n",
      "['germany', 'austria', 'europe', 'german', 'berlin']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=250000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        idf_dic=idf\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                list_of_embeddings = [self.w2v.word2vec[w] for w in sent if w in w2v.word2vec]\n",
    "                if len(list_of_embeddings)!=0:\n",
    "                    sentemb.append(np.mean(list_of_embeddings, axis=0))\n",
    "                    \n",
    "                else:\n",
    "                    #no word is in the lookup table\n",
    "                    sentemb.append(np.zeros(list(self.w2v.word2vec.values())[0].shape))\n",
    "                   \n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                list_of_embeddings = [idf_dic[w]*self.w2v.word2vec[w] for w in sent if w in w2v.word2vec]\n",
    "                if len(list_of_embeddings)!=0: \n",
    "                    sentemb.append(np.mean(list_of_embeddings, axis=0))\n",
    "                else:\n",
    "                    #no word is in the lookup table\n",
    "                    sentemb.append(np.zeros(list(self.w2v.word2vec.values())[0].shape))\n",
    "\n",
    "                   \n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        \n",
    "        scores = np.dot(keys, query.T)\n",
    "        scores /= np.reshape(np.linalg.norm(keys, axis=1), (-1,1))\n",
    "        index_similar_sent = list(np.argsort(scores[:,0])[::-1][:K])\n",
    "        \n",
    "        set_sent = []\n",
    "        print('%s most similar of \"%s\" are \\n' % (K, ' '.join(s)))\n",
    "        if max(scores)>0:\n",
    "            for i,ind in enumerate(index_similar_sent):\n",
    "                print('%s) %s' % (i + 1, ' '.join(sentences[ind])))\n",
    "                set_sent.append(sentences[ind])\n",
    "        else: \n",
    "            print ('The input sentence do not have wordsof the pretrained vocabulary')\n",
    "        return set_sent\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        emb1 = self.encode([s1], idf)\n",
    "        emb2 = self.encode([s2], idf)\n",
    "        if ((np.linalg.norm(emb1)!=0) and ((np.linalg.norm(emb2)!=0))):\n",
    "            return emb1.dot(emb2.T)/(np.linalg.norm(emb1)*np.linalg.norm(emb2))\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1        \n",
    "        for w in idf:\n",
    "            idf[w] = max(1, np.log10(len(sentences) / (idf[w])))\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 pretrained word vectors\n",
      "5 most similar of \"1 smiling african american boy .\" are \n",
      "\n",
      "1) 1 smiling african american boy .\n",
      "2) girl smiling on roller coaster .\n",
      "3) a boy smiles underwater .\n",
      "4) two girlfriends smiling .\n",
      "5) a smiling child swims .\n",
      "[[0.59360354]]\n",
      "\n",
      "\n",
      "\n",
      "5 most similar of \"1 smiling african american boy .\" are \n",
      "\n",
      "1) 1 smiling african american boy .\n",
      "2) 5 women and 1 man are smiling for the camera .\n",
      "3) 2 guys facing away from camera , 1 girl smiling at camera with blue shirt , 1 guy with a beverage with a jacket on .\n",
      "4) two girlfriends smiling .\n",
      "5) 1 man singing and 1 man playing a saxophone in a concert .\n",
      "[[0.50048646]]\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=10000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open(os.path.join(PATH_TO_DATA,'sentences.txt')) as fichier:\n",
    "    sentences = fichier.readlines()\n",
    "\n",
    "sentences = [sent.split() for sent in sentences]\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if False else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13]))\n",
    "print('\\n\\n')\n",
    "\n",
    "#idf = {}  \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print(s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18970\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "intersect = [w for w in w2v_fr.word2vec if w in w2v_en.word2vec]\n",
    "X = np.vstack([w2v_en.word2vec[w] for w in intersect])\n",
    "Y = np.vstack([w2v_fr.word2vec[w] for w in intersect])\n",
    "print(len(intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "import scipy.linalg as la\n",
    "\n",
    "def build_alignement(w2v_lang_1,w2v_lang_2):\n",
    "    #Solve the Procrustes probelm from language 1 to language 2 and give the optimal alinement matrix\n",
    "    intersect = [w for w in w2v_lang_1.word2vec if w in w2v_lang_2.word2vec]\n",
    "    X = np.vstack([w2v_lang_1.word2vec[w] for w in intersect]).T\n",
    "    Y = np.vstack([w2v_lang_2.word2vec[w] for w in intersect]).T\n",
    "    U, s, Vh = la.svd(Y.dot(X.T))\n",
    "    return U.dot(Vh)\n",
    "\n",
    "W = build_alignement(w2v_fr,w2v_en) \n",
    "W_inv = np.linalg.inv(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From French to English\n",
      "Most similar words for fort are :  ['fort', 'forts', 'fortification', 'blockhouse', 'fortifications']\n",
      "Most similar words for chat are :  ['cat', 'rabbit', 'hamster', 'feline', 'poodle']\n",
      "Most similar words for manger are :  ['eat', 'meal', 'eating', 'eaten', 'ate']\n",
      "Most similar words for chien are :  ['dog', 'poodle', 'terrier', 'dogs', 'spaniel']\n",
      "Most similar words for toit are :  ['roof', 'roofed', 'roofs', 'mansard', 'balconies']\n",
      "Most similar words for après are :  ['after', 'shortly', 'afterward', 'afterwards', 'ensuing']\n",
      "\n",
      "\n",
      "From English to French\n",
      "Most similar words for real are :  ['real', 'reality', 'personal', 'réel', 'property']\n",
      "Most similar words for learning are :  ['learning', 'education', 'apprentissage', 'apprentissages', 'educational']\n",
      "Most similar words for dog are :  ['dog', 'chien', 'hound', 'chiens', 'chienne']\n",
      "Most similar words for feeling are :  ['sentiment', 'feeling', 'mélancolie', 'sentiments', 'frustration']\n",
      "Most similar words for tedious are :  ['fastidieux', 'pénible', 'compliqué', 'pénibles', 'inutilement']\n",
      "Most similar words for actually are :  ['would', 'should', 'could', 'that', 'really']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "print('From French to English')\n",
    "\n",
    "for w in ['fort','chat','manger','chien','toit', 'après']:\n",
    "    emb = np.dot(W, w2v_fr.word2vec[w])\n",
    "    print('Most similar words for %s are : ' %  w, str(w2v_en.most_similar(emb)))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('From English to French')\n",
    "for w in [ 'real', 'learning','dog' ,'feeling', 'tedious', 'actually']:\n",
    "    emb = np.dot(W_inv, w2v_en.word2vec[w])\n",
    "    print('Most similar words for %s are : ' % w, str(w2v_fr.most_similar(emb)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def read(file_path,test=False):\n",
    "    #This function allows to read the SST training file\n",
    "    with open(file_path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if test:\n",
    "        return([line.split() for line in lines])\n",
    "    else :\n",
    "        #each line is composed of y_train , x_train\n",
    "        return([line.split()[1:] for line in lines],[int(line.split()[0]) for line in lines])\n",
    "\n",
    "\n",
    "x_train,y_train = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.train'))\n",
    "x_dev,y_dev = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.dev'))\n",
    "x_test = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.test.X'),test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 250000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=250000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "sentences = x_train  + x_dev + x_test\n",
    "idf = {} if False else s2v.build_idf(sentences)\n",
    "\n",
    "x_train_encode = s2v.encode(x_train)\n",
    "\n",
    "x_dev_encode = s2v.encode(x_dev)\n",
    "\n",
    "x_test_encode = s2v.encode(x_test)\n",
    "\n",
    "x_train_encode_idf = s2v.encode(x_train,idf)\n",
    "\n",
    "x_dev_encode_idf = s2v.encode(x_dev,idf)\n",
    "\n",
    "x_test_encode_idf = s2v.encode(x_test,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set without idf precision 0.48408239700374533\n",
      "dev set without idf precision 0.407811080835604\n",
      "train set with idf precision 0.47120786516853935\n",
      "dev set with idf precision 0.4014532243415077\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(x_train_encode)\n",
    "normalized_encoded_train_data = scaler1.transform(x_train_encode)\n",
    "normalized_encoded_valid_data = scaler1.transform(x_dev_encode)\n",
    "\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(x_train_encode_idf)\n",
    "normalized_encoded_train_data_weighted = scaler2.transform(x_train_encode_idf)\n",
    "normalized_encoded_valid_data_weighted = scaler2.transform(x_dev_encode_idf)\n",
    "\n",
    "#without idf\n",
    "clf = LogisticRegression(penalty='l2', C=0.01,tol=1e-6)\n",
    "clf.fit(normalized_encoded_train_data,y_train)\n",
    "y_dev_predict = clf.predict(normalized_encoded_valid_data)\n",
    "y_train_predict = clf.predict(normalized_encoded_train_data)\n",
    "print ('train set without idf precision', accuracy_score(y_train,y_train_predict))\n",
    "print ('dev set without idf precision', accuracy_score(y_dev,y_dev_predict))\n",
    "\n",
    "#with idf\n",
    "clf_idf = LogisticRegression(penalty='l2', C=0.01,tol=1e-6)\n",
    "clf_idf.fit(normalized_encoded_train_data_weighted,y_train)\n",
    "y_dev_predict_idf = clf_idf.predict(normalized_encoded_valid_data_weighted)\n",
    "y_train_predict_idf = clf_idf.predict(normalized_encoded_train_data_weighted)\n",
    "\n",
    "\n",
    "print ('train set with idf precision', accuracy_score(y_train,y_train_predict_idf))\n",
    "print ('dev set with idf precision', accuracy_score(y_dev,y_dev_predict_idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "prediction= clf.predict(x_test_encode)\n",
    "lines = '\\n'.join([str(line) for line in prediction])\n",
    "path_output = os.path.join('.',r'logreg_bov_y_test_sst.txt')\n",
    "with open(path_output,'w') as f:\n",
    "        f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train set without idf precision 0.9774110486891385\n",
      "  dev set without idf precision 0.4069028156221617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train set with idf precision 0.9465121722846442\n",
      "  dev set with idf precision 0.36966394187102636\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "#I tried many classifiers and random forests seems to be the best one \n",
    "print('LGBMC')\n",
    "\n",
    "lgb = LGBMClassifier(max_depth=5,n_estimators=300,reg_lambda=0.01) #with early stopping based on val set\n",
    "\n",
    "lgb.fit(x_train_encode,y_train,verbose=False,early_stopping_rounds=30,eval_metric='multi_logloss',eval_set=[(x_train_encode,y_train),(x_dev_encode,y_dev)]) \n",
    "y_dev_predict = lgb.predict(x_dev_encode)\n",
    "y_train_predict = lgb.predict(x_train_encode)\n",
    "\n",
    "print ('  train set without idf precision', accuracy_score(y_train,y_train_predict))\n",
    "print ('  dev set without idf precision', accuracy_score(y_dev,y_dev_predict))\n",
    "\n",
    "       \n",
    "lgb_idf = LGBMClassifier(max_depth=5,n_estimators=300,reg_lambda=0.01) #with early stopping based on val set\n",
    "\n",
    "lgb_idf.fit(x_train_encode_idf,y_train,verbose=False,early_stopping_rounds=30,eval_metric='multi_logloss',eval_set=[(x_train_encode_idf,y_train),(x_dev_encode_idf,y_dev)])\n",
    "y_dev_predict_idf = lgb_idf.predict(x_dev_encode_idf)\n",
    "y_train_predict_idf = lgb_idf.predict(x_train_encode_idf)\n",
    "    \n",
    "print ('  train set with idf precision', accuracy_score(y_train,y_train_predict_idf))\n",
    "print ('  dev set with idf precision', accuracy_score(y_dev,y_dev_predict_idf))\n",
    "\n",
    "prediction= rf.predict(x_test_encode)\n",
    "lines = '\\n'.join([str(line) for line in prediction])\n",
    "path_output = os.path.join('.',r'lgb_bov_y_test_sst.txt')\n",
    "with open(path_output,'w') as f:\n",
    "        f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = r\"./data/\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "x_train,y_train_temp = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.train'))\n",
    "x_dev,y_dev = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.dev'))\n",
    "x_test = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.test.X'),test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical\n",
    "vocab=np.ravel(np.array(x_train+x_dev+x_test))\n",
    "n=len(vocab)\n",
    "\n",
    "\n",
    "x_train_one_hot = [text.one_hot(' '.join(x),n) for x in x_train]\n",
    "x_dev_one_hot = [text.one_hot(' '.join(x),n) for x in x_dev]\n",
    "x_test_one_hot = [text.one_hot(' '.join(x),n) for x in x_test]\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train_temp)\n",
    "y_val = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_train=len(x_train_one_hot)\n",
    "n_val=len(x_dev_one_hot)\n",
    "pad=pad_sequences(x_train_one_hot+x_dev_one_hot+x_test_one_hot)\n",
    "x_train_pad=pad[:n_train,:]\n",
    "x_val_pad=pad[n_train:n_train+n_val,:]\n",
    "x_test_pad=pad[n_train+n_val:,:]\n",
    "maxseqlen = pad.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(96, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = list(w2v_en.word2vec.values())[0].shape[0] #300  # word embedding dimension\n",
    "nhid       = 96  # number of hidden units in the LSTM\n",
    "vocab_size = n  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 300)         3556500   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 96)                152448    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 485       \n",
      "=================================================================\n",
      "Total params: 3,709,433\n",
      "Trainable params: 3,709,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "from keras import optimizers\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0) # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 41s 5ms/step - loss: 1.4875 - acc: 0.3385 - val_loss: 1.3790 - val_acc: 0.3860\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 1.1501 - acc: 0.4950 - val_loss: 1.4348 - val_acc: 0.4087\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 0.7951 - acc: 0.6879 - val_loss: 1.7132 - val_acc: 0.3733\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 0.4915 - acc: 0.8159 - val_loss: 2.1568 - val_acc: 0.3678\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 0.2946 - acc: 0.8943 - val_loss: 2.5684 - val_acc: 0.3579\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 37s 4ms/step - loss: 0.1696 - acc: 0.9393 - val_loss: 3.0256 - val_acc: 0.3669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "\n",
    "\n",
    "def plot_acc_loss(history):\n",
    "    plt.figure(1)  \n",
    "    plt.figure(figsize=(16,3))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy ' )\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'])\n",
    "   \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "mcp1 = ModelCheckpoint('weights.best.hdf5', monitor=\"val_acc\",save_best_only=True, save_weights_only=False)\n",
    "history = model.fit(x_train_pad, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val_pad, y_val), callbacks = [mcp1])\n",
    "plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "model = keras.models.load_model('weights.best.hdf5') \n",
    "y_test_one_hot = model.predict(x_test_pad)\n",
    "y_test_predict = [np.argmax(y) for y in y_test_one_hot]\n",
    "\n",
    "path_output = os.path.join('.',r'logreg_lstm_y_test_sst.txt')\n",
    "lines = '\\n'.join([str(line) for line in y_test_predict])\n",
    "with open(path_output,'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 52, 300)           3556500   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 52, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 26, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 96)                49536     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 485       \n",
      "=================================================================\n",
      "Total params: 3,635,353\n",
      "Trainable params: 3,635,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/8\n",
      "8544/8544 [==============================] - 31s 4ms/step - loss: 1.5125 - acc: 0.3195 - val_loss: 1.4040 - val_acc: 0.3651\n",
      "Epoch 2/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 1.2927 - acc: 0.4215 - val_loss: 1.4130 - val_acc: 0.3824\n",
      "Epoch 3/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 1.1224 - acc: 0.4951 - val_loss: 1.5087 - val_acc: 0.3942\n",
      "Epoch 4/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.9446 - acc: 0.6010 - val_loss: 1.8178 - val_acc: 0.3497\n",
      "Epoch 5/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.7481 - acc: 0.7065 - val_loss: 1.9959 - val_acc: 0.3588\n",
      "Epoch 6/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.6161 - acc: 0.7674 - val_loss: 2.3482 - val_acc: 0.3560\n",
      "Epoch 7/8\n",
      "8544/8544 [==============================] - 27s 3ms/step - loss: 0.5042 - acc: 0.8097 - val_loss: 2.3277 - val_acc: 0.3479\n",
      "Epoch 8/8\n",
      "8544/8544 [==============================] - 28s 3ms/step - loss: 0.4280 - acc: 0.8454 - val_loss: 2.3717 - val_acc: 0.3460\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation,Dropout,Conv1D,Flatten, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional \n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = r\"./data/\"\n",
    "\n",
    "\n",
    "x_train,y_train_temp = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.train'))\n",
    "x_dev,y_dev_temp = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.dev'))\n",
    "x_test = read(os.path.join(PATH_TO_DATA,'SST','stsa.fine.test.X'),test=True)\n",
    "\n",
    "vocab=np.ravel(np.array(x_train+x_dev+x_test))\n",
    "n=len(vocab)\n",
    "\n",
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "x_train_one_hot = [text.one_hot(' '.join(x),n) for x in x_train]\n",
    "x_dev_one_hot = [text.one_hot(' '.join(x),n) for x in x_dev]\n",
    "x_test_one_hot = [text.one_hot(' '.join(x),n) for x in x_test]\n",
    "\n",
    "y_train = to_categorical(y_train_temp)\n",
    "y_val = to_categorical(y_dev_temp)\n",
    "\n",
    "\n",
    "\n",
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "n_train=len(x_train_one_hot)\n",
    "n_val=len(x_dev_one_hot)\n",
    "pad=pad_sequences(x_train_one_hot+x_dev_one_hot+x_test_one_hot)\n",
    "x_train_pad=pad[:n_train,:]\n",
    "x_val_pad=pad[n_train:n_train+n_val,:]\n",
    "x_test_pad=pad[n_train+n_val:,:]\n",
    "maxseqlen = pad.shape[1]\n",
    "\n",
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "vocab_size = n # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "list_w = set([w for sent in (x_train+x_dev+x_test) for w in sent ])\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=10000000)\n",
    "for w in list_w:\n",
    "    if w in w2v.word2vec:\n",
    "        embedding_matrix[text.one_hot(' '.join(w),vocab_size)] = w2v.word2vec[w]\n",
    "\n",
    "\n",
    "mcp1 = ModelCheckpoint('weights.best_innovate.hdf5', monitor=\"val_acc\",save_best_only=True, save_weights_only=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embed_dim,input_length=maxseqlen,weights=[embedding_matrix]))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(96,dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(vocab_size,embed_dim,weights=[embedding_matrix]))\n",
    "#model.add(Conv1D(64, 3, activation='relu', input_shape=(vocab_size, embed_dim)))\n",
    "#model.add(Conv1D(64, 3, activation='relu'))\n",
    "#model.add(MaxPooling1D(3))\n",
    "#model.add(Conv1D(128, 3, activation='relu'))\n",
    "#model.add(Conv1D(128, 3, activation='relu'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "bs = 64\n",
    "n_epochs = 8\n",
    "\n",
    "\n",
    "history = model.fit((x_train_pad), y_train, batch_size=bs, epochs=n_epochs, validation_data=((x_val_pad), y_val),callbacks = [mcp1])\n",
    "\n",
    "y_test_one_hot = model.predict(x_test_pad)\n",
    "y_test_predict = [np.argmax(y) for y in y_test_one_hot]\n",
    "\n",
    "path_output = os.path.join('.',r'conv1d_y_test_sst.txt')\n",
    "lines = '\\n'.join([str(line) for line in y_test_predict])\n",
    "with open(path_output,'w') as f:\n",
    "    f.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
